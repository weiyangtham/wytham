---
title: Scraping NIH PIs with Rvest
author: Wei Yang
date: '2018-02-15'
draft: true
slug: scraping-nih-pis-with-rvest
categories:
  - R
tags:
  - R
---

Background: I was doing some exploratory work for a potential project looking at intramural investigators at the NIH. Eventually I decided to put it aside for the time being, but here is some cleaned up code from exploratory phase. 

# The Big Picture

The NIH is usually known for its funding of biomedical research in universities and other research institutions i.e. its *extramural* program. However, the NIH also employs its own scientists - this is its *intramural* research program or IRP. The goal of this post is to see if we can use [publicly available data](https://www.buzzfeed.com/jsvine/sharing-hundreds-of-millions-of-federal-payroll-records?utm_term=.gwyZO2Aak#.xt8KRex5A) on government employees (more on that in a bit) to learn something about that population. 

I'm going to do this in two broad steps:

1. Get the names of IRP investigators
2. Use the names to link to federal employee data from the Office of Personnel Management [(courtesty of Buzzfeed)](https://www.buzzfeed.com/jsvine/sharing-hundreds-of-millions-of-federal-payroll-records?utm_term=.gwyZO2Aak#.xt8KRex5A)

# Scraping Investigator Names

You can find the name of all IRP Principal Investigators (PIs) [here](https://irp.nih.gov/our-research/principal-investigators/name), or see a screenshot of the page below. 

![](/img/posts/2018-02-15-scraping-nih-pis-with-rvest/pi_names.png)

I want to collect all these names without having to copy and paste them. This will help make the analysis reproducible and less prone to error. I'm going to do this with the `rvest` package, following the steps in [the vignette for SelectorGadget](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html).

> Selectorgadget is a javascript bookmarklet that allows you to interactively figure out what css selector you need to extract desired components from a page.

What I need to do is go to [the page I want to scrape data from](https://irp.nih.gov/our-research/principal-investigators/name) and use SelectorGadget to select the elements I want (yellow highlight) and don't want (red highlight). I end up with this: 

![](/img/posts/2018-02-15-scraping-nih-pis-with-rvest/selectorgadget.png)

At the bottom of the screenshot, SelectorGadget has determined the right CSS selector needed to extract the names. Now we can move to doing everything in R. 

```{r, eval = TRUE, message = FALSE, warning = FALSE}
library(tidyverse)
library(magrittr)
library(rvest)

html = read_html("https://irp.nih.gov/our-research/principal-investigators/name")

# based on selectorgadget
pi_names_html = html_nodes(html, ".pilist-name a")

pi_names = html_text(pi_names_html) 

pi_names[1:5]
```

We now have the PI names as a character vector along with their degrees. To keep things simple, I'm going to extract just their first and last names and ignore the other information. 

```{r}
df = data_frame(fullname = pi_names)

# separate name into first, middle, last
df %<>%
  tidyr::extract(fullname, into = c("last_name", "first_name"), 
                 regex = "(.+?)\\,\\s(.+?)\\,", remove = FALSE)  %>% 
  separate(first_name, into = c("first_name", "middle_name"), sep = "\\s") 

df

```

## Clean names

Below is some code for standardizing the names. I do three things:

1. convert to lower case
2. replace accented characters with plain ones (e.g. é to e)
3. replace non-letters with spaces (e.g. O'Neill to oneill)

```{r, eval=TRUE}

# Lower case
df %<>% mutate_at(vars(last_name:middle_name), str_to_lower)

# Replace accents with plain letters
accent_dictionary = 
  list('Š'='S', 'š'='s', 'Ž'='Z', 'ž'='z', 'À'='A', 'Á'='A', 'Â'='A', 'Ã'='A', 'Ä'='A', 'Å'='A', 'Æ'='A', 'Ç'='C', 'È'='E', 'É'='E',
       'Ê'='E', 'Ë'='E', 'Ì'='I', 'Í'='I', 'Î'='I', 'Ï'='I', 'Ñ'='N', 'Ò'='O', 'Ó'='O', 'Ô'='O', 'Õ'='O', 'Ö'='O', 'Ø'='O', 'Ù'='U',
       'Ú'='U', 'Û'='U', 'Ü'='U', 'Ý'='Y', 'Þ'='B', 'ß'='Ss', 'à'='a', 'á'='a', 'â'='a', 'ã'='a', 'ä'='a', 'å'='a', 'æ'='a', 'ç'='c',
       'è'='e', 'é'='e', 'ê'='e', 'ë'='e', 'ì'='i', 'í'='i', 'î'='i', 'ï'='i', 'ð'='o', 'ñ'='n', 'ò'='o', 'ó'='o', 'ô'='o', 'õ'='o',
       'ö'='o', 'ø'='o', 'ù'='u', 'ú'='u', 'û'='u', 'ü'='u', 'ý'='y', 'ý'='y', 'þ'='b', 'ÿ'='y')

df %<>%
  mutate(first_name = 
           chartr(paste(names(accent_dictionary), collapse = ''),
                  paste(accent_dictionary, collapse = ''), first_name), 
  last_name = chartr(paste(names(accent_dictionary), collapse = ''),
                     paste(accent_dictionary, collapse = ''), last_name))

# Replace non-letters 
df %<>% mutate(first_name = str_replace_all(first_name, "[^a-z]", ""), 
              last_name = str_replace_all(last_name, "[^a-z]", "")) 

```


# OPM data 

Now that we have the names of the IRP PIs, let's see if we can identify them in this awesome dataset that [Buzzfeed put online for public use](https://www.buzzfeed.com/jsvine/sharing-hundreds-of-millions-of-federal-payroll-records?utm_term=.gwyZO2Aak#.xt8KRex5A).

> The dataset contains hundreds of millions of rows and stretches all the way back to 1973. It provides salary, title, and demographic details about millions of U.S. government employees, as well as their migrations into, out of, and through the federal bureaucracy. In many cases, the data also contains employees’ names.

Thank you, Buzzfeed!

I have the data stored on an external hard drive, so for this part of the code you'll have to change the file path to wherever you've downloaded the data. 

```{r, eval = T}
# Where you've stored the Buzzfeed data after downloading it
your_root = "/Volumes/research_data"

path = file.path(your_root, "opm-federal-employment-data/data", 
       "2016-12-to-2017-03/non-dod/status", "Non-DoD FOIA 2017-04762 201703.txt")

opm2017 = read_delim(path, col_types = cols(), delim = ";", 
                     na = c("", "NA", "*", ".", "############"), n_max = Inf) 

# install.packages("janitor")
opm2017 %<>% janitor::clean_names() # clean column names

opm2017
```

I perform the same operations for cleaning names. I also `filter` the employees belonging to the NIH using the `subagency` code "HE38". 

```{r, eval = TRUE}

# NIH employees only
opm2017 %<>% filter(str_detect(subagency, "HE38"))

# clean names
opm2017 %<>%
  mutate_at(vars(last_name, first_name), str_to_lower) %>%  # set names to lower case
  mutate(first_name = str_replace_all(first_name, "[^a-z]", ""),
       last_name = str_replace_all(last_name, "[^a-z]", "")) %>% # replace non-letters with space
  mutate(first_name =
           chartr(paste(names(accent_dictionary), collapse=''),
                  paste(accent_dictionary, collapse=''), first_name),
         last_name = 
           chartr(paste(names(accent_dictionary), collapse=''),
                  paste(accent_dictionary, collapse=''), last_name))

```

## Linking PI names to OPM payroll data

I'm going to implement a simple matching rule: unique matches on first and last names. 

```{r, eval = TRUE}

# Give IDs to track duplicates
df %<>% mutate(id = row_number()) %>% select(id, everything())
opm2017 %<>% mutate(opm_id = row_number()) %>% select(opm_id, everything())

pi_opm = df %>% select(id, last_name, first_name) %>% 
  inner_join(opm2017, by = c("last_name", "first_name"))

pi_opm %<>% 
  group_by(id) %>% 
  mutate(x = max(row_number()) > 1) %>% # duplicate from dataframe of scraped names
  group_by(opm_id) %>% 
  mutate(y = max(row_number()) > 1) %>% # duplicate from OPM data
  ungroup() %>% 
  filter(!x, !y)

```

We had `r nrow(df)` names and managed to match `r nrow(pi_opm)` of them. We could maybe do better but for such a simple "algorithm" I'm happy enough. 

# What can we learn about the intramural program?

It's a good idea to take a peek at the data with the wonderful [`skimr`](https://github.com/ropenscilabs/skimr) package, even if we will plot some of the variables again later. Age and years of service are reported in 5-year bins, so to I've converted them into numeric values based on the lower end of the bin. 

```{r, warning = FALSE}

pi_opm %<>% mutate(age = parse_number(age_range), yrs_since_degree = parse_number(ysd_range))

skimr::skim(pi_opm)

```

## Aging of the scientifc workforce

A prominent question about the scientific research workforce these days is whether it is aging [(yes)](http://www.pnas.org/content/114/15/3879) and the implications of the aging phenomenon for scientific research. 

```{r}
library(hrbrthemes)
theme_set(theme_ipsum(base_size = 18))

pi_opm %>% 
  ggplot(aes(age)) + geom_bar() + 
  labs(title = "Intramural investigators seem old")
```

The distribution of the intramural PIs is older than I'd have guessed. Strikingly, PIs older than 65 make up the largest share of the population. It'd be interesting to see how this distribution has changed over time. While this may be related to the general aging phenomenon, research funding or hiring policies could also have an effect. 

## How much do scientists earn?

```{r, message = FALSE, warning = FALSE}
pi_opm %>% 
  ggplot(aes(adjusted_basic_pay)) + 
  geom_histogram() 
```

Not a bad living, but there also seems to be a fairly wide range. Some of this might be down to experience, which we can check by comparing salary with years since degree. 

```{r}
pi_opm %>% 
  ggplot(aes(yrs_since_degree, adjusted_basic_pay)) + 
  geom_jitter()
```




